{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df0879a-8a5f-4077-90fd-be2ed9e43ab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importa√ß√£o de dados e bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4b179-52eb-45b5-9016-faff5070073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona a pasta raiz do projeto ao sys.path\n",
    "import sys\n",
    "if \"/home/user\" not in sys.path:\n",
    "    sys.path.append(\"/home/user\")\n",
    "from src.spark_session import create_spark_session\n",
    "spark = create_spark_session()\n",
    "\n",
    "\n",
    "## Importar a sess√£o spark criada no spark_session.py\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949715c-73be-4c40-84a3-e37d75fe3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas todos os meses que devem ser extra√≠dos os dados.\n",
    "client = Minio(\n",
    "    \"minio:9000\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio123\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "bucket_name = \"bronze\"\n",
    "\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "months_to_extract = list(set([obj.object_name.split('/')[2] for obj in objects]))\n",
    "\n",
    "if len(months_to_extract) > 0:\n",
    "    \n",
    "    # Ler cada arquivo e unir em um √∫nico dataframe\n",
    "    df_union_all = None\n",
    "    print(f\"Iniciando processo de ler os dados do bucket.\")\n",
    "    for fold in months_to_extract:\n",
    "        ## Defini√ß√£o da path\n",
    "        path = f\"s3a://bronze/uber_dataset/{datetime.now().month}_{datetime.now().year}/{fold}\"\n",
    "\n",
    "        df_temp = spark.read.format('parquet').load(path)\n",
    "        print(f\"üìÑDados referentes ao m√™s de {fold} transformado em Dataframe.\")\n",
    "        if df_union_all == None:\n",
    "            df_union_all = df_temp\n",
    "        else:\n",
    "            df_union_all = df_union_all.unionAll(df_temp)\n",
    "    print(f\"‚úÖ Dados importados e unidos.\")\n",
    "else:\n",
    "    print(f\"‚ùå Erro: na Leitura do bucket\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b4b0b-14e9-4640-961e-b8654e83dfc6",
   "metadata": {},
   "source": [
    "## Tratamento dos dados\n",
    "\n",
    "abaixo utilizei duas abordagens diferentes, a primeira mais tradicional utilizando withcolumns, por√©m em quest√£o perform√°ticas ela acaba sendo n√£o t√£o efici√™nte no processamento desses dados, ent√£o abaixo demosntro a segunda op√ß√£o que acelera o processamento desses dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627cc8b-8a92-450f-abfb-84b9de3df76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma fun√ß√£o de cria√ß√£o de UDF \n",
    "def criar_udf_mapeamento(dicionario):\n",
    "    return f.udf(lambda valor: dicionario.get(valor), StringType())\n",
    "\n",
    "# Definir Mapas\n",
    "days_of_week = {\n",
    "    1: \"Domingo\",\n",
    "    2: \"Segunda-feira\",\n",
    "    3: \"Ter√ßa-feira\",\n",
    "    4: \"Quarta-feira\",\n",
    "    5: \"Quinta-feira\",\n",
    "    6: \"Sexta-feira\",\n",
    "    7: \"S√°bado\",\n",
    "}\n",
    "\n",
    "base = {\n",
    "    'B02512':'A',\n",
    "    'B02598':'B',\n",
    "    'B02682':'D',\n",
    "    'B02617':'C',\n",
    "    'B02764':'E',\n",
    "}\n",
    "\n",
    "# Criar UDFs para o de<>para necess√°rio de dia da semana e Base\n",
    "udf_dia = criar_udf_mapeamento(days_of_week)\n",
    "udf_base = criar_udf_mapeamento(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beaf416-a96a-4915-b2dc-0c53d0f634d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tradition = (\n",
    "    df_union_all\n",
    "    .withColumnRenamed('Lat','latitude')\n",
    "    .withColumnRenamed('Lon','longitude')\n",
    "    .withColumnRenamed('Base','base_associada')\n",
    "    .withColumn('data',f.to_timestamp('Date/Time','M/d/yyyy H:mm:ss'))\n",
    "    .withColumn('month',f.month('data'))\n",
    "    .withColumn('year',f.year('data'))\n",
    "    .withColumn('time',f.date_format('data', \"HH:mm:ss\"))\n",
    "    .withColumn('week', udf_dia(f.dayofweek('data')))\n",
    "    .withColumn('base_associada', udf_base('base_associada'))\n",
    "    .withColumn('extraction_date', f.lit(datetime.now().date()))\n",
    ").select('latitude','longitude','base_associada','month','year','time','week','extraction_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f40f5-e15a-4add-89d5-9f1437ed4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performatic = df_union_all.select(\n",
    "        f.col(\"Lat\").alias(\"latitude\"),\n",
    "        f.col(\"Lon\").alias(\"longitude\"),\n",
    "        f.col(\"Base\").alias('base_associada'),\n",
    "        f.to_timestamp(\"Date/Time\", \"M/d/yyyy H:mm:ss\").alias(\"data\")\n",
    ").select(\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        udf_base('base_associada').alias('base_associada'),\n",
    "        f.month('data').alias(\"month\"),\n",
    "        f.year('data').alias(\"year\"),\n",
    "        f.date_format('data', \"HH:mm:ss\").alias(\"time\"),\n",
    "        udf_dia(f.dayofweek('data')).alias('week'),\n",
    "        f.lit(datetime.now().date()).alias(\"extraction_date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a617fc-7c67-4aa1-bb21-4c3006bd5a55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exportar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e83488-e55d-40fb-87a7-9951ecec6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o de como salvar os arquivos\n",
    "bucket = 'silver'\n",
    "project_name = 'uber_dataset'\n",
    "\n",
    "save_path = f\"s3a://{bucket}/{project_name}/{datetime.now().month}_{datetime.now().year}\"\n",
    "\n",
    "df_performatic.write\\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"base_associada\") \\\n",
    "    .format('delta')\\\n",
    "    .save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba40da2-d0ff-4094-8a6c-82d2f6b19953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o de como salvar os arquivos\n",
    "bucket = 'silver'\n",
    "project_name = 'geo_spatial_infos'\n",
    "\n",
    "save_path = f\"s3a://{bucket}/{project_name}/{datetime.now().month}_{datetime.now().year}\"\n",
    "\n",
    "df_performatic.write\\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format('delta')\\\n",
    "    .save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
